{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d7542153",
      "metadata": {
        "id": "d7542153"
      },
      "source": [
        "## Transformer\n",
        "\n",
        "Transformers only reads in the following formats:\n",
        "\n",
        "1. txt\n",
        "2. csv\n",
        "3. tsv\n",
        "4. jsonl\n",
        "5. json\n",
        "6. xml\n",
        "\n",
        "[Source](https://huggingface.co/docs/datasets/dataset_script.html)\n",
        "\n",
        "If you want to read in pdf files, you would have to convert it to any of the format above.\n",
        "\n",
        "for our notebook we would be converting them to pdf."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install pdfminer\n",
        "!pip install rouge_score\n",
        "!pip install rouge"
      ],
      "metadata": {
        "id": "GTTiPiy5VZLh"
      },
      "id": "GTTiPiy5VZLh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "788ceb0e",
      "metadata": {
        "id": "788ceb0e"
      },
      "outputs": [],
      "source": [
        "from io import StringIO\n",
        "\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfdocument import PDFDocument\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from pdfminer.pdfparser import PDFParser\n",
        "from datasets import load_metric\n",
        "\n",
        "\n",
        "\n",
        "#from transformers import pipeline\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "cf69f132",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf69f132",
        "outputId": "dcd5a7ba-340e-48e9-f5f8-d67339970b9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester\n",
            "County, New York.\n",
            "A year later, she got married again in Westchester County, but to a different man and without\n",
            "divorcing her first husband.\n",
            "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five\n",
            "more times, sometimes only within two weeks of each other.\n",
            "In 2010, she married once more, this time in the Bronx. In an application for a marriage license,\n",
            "she stated it was her \"first and only\" marriage.\n",
            "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the\n",
            "first degree,\" referring to her false statements on the\n",
            "2010 marriage license application, according to court documents.\n",
            "Prosecutors said the marriages were part of an immigration scam.\n",
            "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her\n",
            "attorney, Christopher Wright, who declined to comment further.\n",
            "After leaving court, Barrientos was arrested and charged with theft of service and criminal\n",
            "trespass for allegedly sneaking into the New York subway through an emergency exit, said\n",
            "Detective\n",
            "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with\n",
            "nine of her marriages occurring between 1999 and 2002.\n",
            "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is\n",
            "believed to still be married to four men, and at one time, she was married to eight men at once,\n",
            "prosecutors say.\n",
            "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent\n",
            "residence status shortly after the marriages.\n",
            "Any divorces happened only after such filings were approved. It was unclear whether any of the\n",
            "men will be prosecuted.\n",
            "The case was referred to the Bronx District Attorney\\'s Office by Immigration and Customs\n",
            "Enforcement and the Department of Homeland Security\\'s\n",
            "Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including\n",
            "Egypt, Turkey, Georgia, Pakistan and Mali.\n",
            "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an\n",
            "investigation by the Joint Terrorism Task Force.\n",
            "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is\n",
            "scheduled for May 18.\n",
            "\n",
            "\f\n"
          ]
        }
      ],
      "source": [
        "output_string = StringIO()\n",
        "with open(\"data/summary text.pdf\" , \"rb\") as in_file:\n",
        "    parser = PDFParser(in_file)\n",
        "    doc = PDFDocument(parser)\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    device =  TextConverter(rsrcmgr, output_string, laparams= LAParams())\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    \n",
        "    for page in PDFPage.create_pages(doc):\n",
        "        interpreter.process_page(page)\n",
        "        #print('*' * 100)\n",
        "        \n",
        "print(output_string.getvalue())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cf5081df",
      "metadata": {
        "id": "cf5081df"
      },
      "outputs": [],
      "source": [
        "metric = load_metric('rouge')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "2c5e792d",
      "metadata": {
        "id": "2c5e792d"
      },
      "outputs": [],
      "source": [
        "text = output_string.getvalue()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "7969000d",
      "metadata": {
        "id": "7969000d"
      },
      "outputs": [],
      "source": [
        "text = text[:-4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "48196eb6",
      "metadata": {
        "id": "48196eb6"
      },
      "outputs": [],
      "source": [
        "dataset = text.replace('\\n', ' ').replace('\\\\', '').replace('\\n\\n', '').replace('\\x0c','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "a98ce36e",
      "metadata": {
        "id": "a98ce36e"
      },
      "outputs": [],
      "source": [
        "n =512\n",
        "bank = [dataset[i:i+n] for i in range(0, len(dataset), n)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "9af7f2a1",
      "metadata": {
        "id": "9af7f2a1"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "3a84db51",
      "metadata": {
        "id": "3a84db51"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "50648403",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50648403",
        "outputId": "716c62c3-9632-489b-9347-6425249bcf4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " liana barrientos got married in the westchester county, new york, in 2010. she got married again in the westchester county, but to a different man. in 2010, she married once more, this time in the Bronx. barrientos is accused of \"offering a false instrument for filing in the first degree\" she pleaded not guilty to two counts of \"offering a false instrument for filing in the first degree\" the marriages were part of an immigration scam, she says. in total, barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002. she is believed to still be married to four men, and at one time, she was married to eight men at once. the case was referred to the Bronx District Attorney's Office. seven of the men are from so-called \"red-flagged\" countries. it is unclear whether any of the men will be prosecuted. barrientos faces up to four years in prison if convicted. her eighth husband, Rashid Rajput, was deported to his native Pakistan in 2006. if convicted, she faces up to four years in prison.\n"
          ]
        }
      ],
      "source": [
        "summary = []\n",
        "for data in bank:\n",
        "    #inputs, references = batch\n",
        "    inputs = tokenizer(\"summarize: \" + data, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=150, min_length=40,length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary.append(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "\n",
        "\n",
        "    #print(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "result  = \"\".join(summary)\n",
        "result = result.replace(r'<pad>','').replace(r'</s>','')\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "metric.compute(predictions = [result], references = [dataset])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q921fxUSVsaA",
        "outputId": "ba38342b-f13d-4a61-ec12-ec968126829c"
      },
      "id": "q921fxUSVsaA",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.9347826086956522, recall=0.45989304812834225, fmeasure=0.6164874551971327), mid=Score(precision=0.9347826086956522, recall=0.45989304812834225, fmeasure=0.6164874551971327), high=Score(precision=0.9347826086956522, recall=0.45989304812834225, fmeasure=0.6164874551971327)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.7213114754098361, recall=0.353887399463807, fmeasure=0.4748201438848921), mid=Score(precision=0.7213114754098361, recall=0.353887399463807, fmeasure=0.4748201438848921), high=Score(precision=0.7213114754098361, recall=0.353887399463807, fmeasure=0.4748201438848921)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.7554347826086957, recall=0.3716577540106952, fmeasure=0.4982078853046595), mid=Score(precision=0.7554347826086957, recall=0.3716577540106952, fmeasure=0.4982078853046595), high=Score(precision=0.7554347826086957, recall=0.3716577540106952, fmeasure=0.4982078853046595)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.7554347826086957, recall=0.3716577540106952, fmeasure=0.4982078853046595), mid=Score(precision=0.7554347826086957, recall=0.3716577540106952, fmeasure=0.4982078853046595), high=Score(precision=0.7554347826086957, recall=0.3716577540106952, fmeasure=0.4982078853046595))}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "r = Rouge()\n",
        "r.get_scores(result, dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60kbjP2sWFlP",
        "outputId": "ba3e9917-a67e-4ab1-da37-5b250add4df4"
      },
      "id": "60kbjP2sWFlP",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rouge-1': {'f': 0.5266272146974896,\n",
              "   'p': 0.8640776699029126,\n",
              "   'r': 0.37872340425531914},\n",
              "  'rouge-2': {'f': 0.40725806024982114,\n",
              "   'p': 0.6601307189542484,\n",
              "   'r': 0.2944606413994169},\n",
              "  'rouge-l': {'f': 0.5147928951708624,\n",
              "   'p': 0.8446601941747572,\n",
              "   'r': 0.3702127659574468}}]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_LXC_Kl-WE9I"
      },
      "id": "_LXC_Kl-WE9I",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0014d271",
      "metadata": {
        "id": "0014d271"
      },
      "source": [
        "### Trying out new model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "47541ebb",
      "metadata": {
        "id": "47541ebb"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "e4f986aa",
      "metadata": {
        "id": "e4f986aa"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "40810959",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40810959",
        "outputId": "b91fdca4-f302-4640-eda2-ca398f0ad70b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " a year later, she got married again in westchester county, new york. only 18 days after that marriage, she got hitched yet again. in 2010, she married once more, this time in the Bronx. barrientos, now 39, is facing two criminal counts of \"offering a false instrument\" referring to her false statements on the 2010 marriage license application. the marriages were part of an immigration scam, her attorney says. in total, barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002. she is believed to still be married to four men, and at one time, she was married to eight men at once. seven of the men are from so-called \"red-flagged\" countries. seven of the men are from so-called \"red-flagged\" countries. the case was referred to the district attorney's office. if convicted, barrientos faces up to four years in prison. if convicted, barrientos faces up to four years in prison. next court appearance is scheduled for may 18.\n"
          ]
        }
      ],
      "source": [
        "summary = []\n",
        "for datas in bank:\n",
        "    inputs = tokenizer(\"summarize: \" + datas, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(\n",
        "    inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary.append(tokenizer.decode(outputs[0]))\n",
        "    #print(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "result  = \"\".join(summary)\n",
        "result = result.replace(r'<pad>','').replace(r'</s>','')\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric.compute(predictions = [result], references = [dataset])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5_EtblJMHMa",
        "outputId": "78c0322c-fd40-475d-dafe-dab53a2d4bcd"
      },
      "id": "Z5_EtblJMHMa",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.9112426035502958, recall=0.4117647058823529, fmeasure=0.567219152854512), mid=Score(precision=0.9112426035502958, recall=0.4117647058823529, fmeasure=0.567219152854512), high=Score(precision=0.9112426035502958, recall=0.4117647058823529, fmeasure=0.567219152854512)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.8035714285714286, recall=0.36193029490616624, fmeasure=0.4990757855822551), mid=Score(precision=0.8035714285714286, recall=0.36193029490616624, fmeasure=0.4990757855822551), high=Score(precision=0.8035714285714286, recall=0.36193029490616624, fmeasure=0.4990757855822551)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.834319526627219, recall=0.3770053475935829, fmeasure=0.5193370165745856), mid=Score(precision=0.834319526627219, recall=0.3770053475935829, fmeasure=0.5193370165745856), high=Score(precision=0.834319526627219, recall=0.3770053475935829, fmeasure=0.5193370165745856)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.834319526627219, recall=0.3770053475935829, fmeasure=0.5193370165745856), mid=Score(precision=0.834319526627219, recall=0.3770053475935829, fmeasure=0.5193370165745856), high=Score(precision=0.834319526627219, recall=0.3770053475935829, fmeasure=0.5193370165745856))}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#with beam\n",
        "from rouge import Rouge\n",
        "r = Rouge()\n",
        "r.get_scores(result, dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gajtta-cZzbW",
        "outputId": "d085060b-71ab-48ea-8e0c-3d57b7374858"
      },
      "id": "gajtta-cZzbW",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'rouge-1': {'f': 0.5088757354075488,\n",
              "   'p': 0.8349514563106796,\n",
              "   'r': 0.3659574468085106},\n",
              "  'rouge-2': {'f': 0.4123711298793921,\n",
              "   'p': 0.704225352112676,\n",
              "   'r': 0.2915451895043732},\n",
              "  'rouge-l': {'f': 0.5029585756442352,\n",
              "   'p': 0.8252427184466019,\n",
              "   'r': 0.3617021276595745}}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test on another pdf"
      ],
      "metadata": {
        "id": "iSwDs4N6Z0w5"
      },
      "id": "iSwDs4N6Z0w5"
    },
    {
      "cell_type": "code",
      "source": [
        "output_string = StringIO()\n",
        "with open(\"data/summary_one.pdf\" , \"rb\") as in_file:\n",
        "    parser = PDFParser(in_file)\n",
        "    doc = PDFDocument(parser)\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    device =  TextConverter(rsrcmgr, output_string, laparams= LAParams())\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    \n",
        "    for page in PDFPage.create_pages(doc):\n",
        "        interpreter.process_page(page)\n",
        "        #print('*' * 100)\n",
        "        \n",
        "print(output_string.getvalue())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XS7S2rK_URUF",
        "outputId": "bcef8d77-bf9a-476d-9dab-e66d0ed398d5"
      },
      "id": "XS7S2rK_URUF",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There were certain underlying conditions that enabled department stores to\n",
            "grow up when they did. From the start, they all catered for middle-class\n",
            "customers and set out to convey to them an air of luxury and solid comfort. Of\n",
            "necessity, they all arose in central positions where large numbers of people\n",
            "could reach them easily by means of public transport. Physically, they grew up\n",
            "in an era of big technical developments in building so that they could a\u0000ord\n",
            "multi-storey palaces and could have enormous plate-glass windows for\n",
            "display, lighting and novelties like lifts.\n",
            "Above all, the department stores rose with the rise of Victorian white-collar\n",
            "workers, the small-scale businessmen and professionals whose womenfolk\n",
            "had money to spare for a few luxuries and were gradually switching the\n",
            "emphasis of their housekeeping expenditure from food to other items.\n",
            "\n",
            "Most of these stores drew enough customers to ﬁll their huge shops by o\u0000ering\n",
            "two new things. One was the new manufactures, particularly clothing, goods\n",
            "and accessories, household furnishings and equipment of all kinds that were\n",
            "coming out of the factories in increasing quantities. The specialist shops\n",
            "stocked these too, of course, but the department stores always made it a point\n",
            "to be the ﬁrst in the ﬁeld if they could with novelty of any kind. And the other\n",
            "special thing they o\u0000ered the middle-class shoppers, many of whom were\n",
            "newly rich and a little experienced in luxury shopping, was a lavish display and\n",
            "wide choice of these goods.\n",
            "\n",
            "The department stores, however, introduced into a respectable class trade the\n",
            "vulgar practice of openly marking or ticketing goods with their prices – a\n",
            "practice that had not even yet penetrated shops that could claim that they were\n",
            "really exclusive. But the department stores as a rule made a virtue not only of\n",
            "displaying their wares as openly as they could but also of boldly pricing them\n",
            "for all to see. Their large-scale purchases enabled them to sell cheaply and they\n",
            "were not ashamed in the early days to make price one of their selling points.\n",
            "‘Store price’ was a by-word for cheapness.\n",
            "\n",
            "The lines they concentrated on were fashion goods, things that shoppers were\n",
            "prepared to travel long distances for and to take some time and trouble in\n",
            "choosing. The department stores were at least partly responsible for the way\n",
            "the middle classes gradually became fashion conscious, and helped to mould\n",
            "their tastes. They were the ﬁrst preachers of the modern creed that goods\n",
            "\n",
            "\fought to be replaced when they are outdated rather than when they are\n",
            "outworn.\n",
            "\n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = output_string.getvalue()\n",
        "text = text[:-4]\n",
        "dataset = text.replace('\\n', ' ').replace('\\\\', '').replace('\\n\\n', '').replace('\\x0c','').replace('\\x00', 'ff')\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "ApnxIAnCVj0O",
        "outputId": "cdade30f-f193-4d5c-bc2a-e1c2f541f653"
      },
      "id": "ApnxIAnCVj0O",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'There were certain underlying conditions that enabled department stores to grow up when they did. From the start, they all catered for middle-class customers and set out to convey to them an air of luxury and solid comfort. Of necessity, they all arose in central positions where large numbers of people could reach them easily by means of public transport. Physically, they grew up in an era of big technical developments in building so that they could afford multi-storey palaces and could have enormous plate-glass windows for display, lighting and novelties like lifts. Above all, the department stores rose with the rise of Victorian white-collar workers, the small-scale businessmen and professionals whose womenfolk had money to spare for a few luxuries and were gradually switching the emphasis of their housekeeping expenditure from food to other items.  Most of these stores drew enough customers to ﬁll their huge shops by offering two new things. One was the new manufactures, particularly clothing, goods and accessories, household furnishings and equipment of all kinds that were coming out of the factories in increasing quantities. The specialist shops stocked these too, of course, but the department stores always made it a point to be the ﬁrst in the ﬁeld if they could with novelty of any kind. And the other special thing they offered the middle-class shoppers, many of whom were newly rich and a little experienced in luxury shopping, was a lavish display and wide choice of these goods.  The department stores, however, introduced into a respectable class trade the vulgar practice of openly marking or ticketing goods with their prices – a practice that had not even yet penetrated shops that could claim that they were really exclusive. But the department stores as a rule made a virtue not only of displaying their wares as openly as they could but also of boldly pricing them for all to see. Their large-scale purchases enabled them to sell cheaply and they were not ashamed in the early days to make price one of their selling points. ‘Store price’ was a by-word for cheapness.  The lines they concentrated on were fashion goods, things that shoppers were prepared to travel long distances for and to take some time and trouble in choosing. The department stores were at least partly responsible for the way the middle classes gradually became fashion conscious, and helped to mould their tastes. They were the ﬁrst preachers of the modern creed that goods  ought to be replaced when they are outdated rather than when they are outworn'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n =512\n",
        "bank = [dataset[i:i+n] for i in range(0, len(dataset), n)]\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "\n",
        "summary = []\n",
        "for data in bank:\n",
        "    #inputs, references = batch\n",
        "    inputs = tokenizer(\"summarize: \" + data, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary.append(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "\n",
        "\n",
        "    #print(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "result  = \"\".join(summary)\n",
        "result = result.replace(r'<pad>','').replace(r'</s>','')\n",
        "\n",
        "print(result)\n",
        "\n",
        "metric.compute(predictions = [result], references = [dataset])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA0qiX1QUROt",
        "outputId": "24f2a523-d460-49b2-ea91-920c7d83eb0c"
      },
      "id": "ZA0qiX1QUROt",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " liana barrientos got married in the westchester county, new york, in 2010. she got married again in the westchester county, but to a different man. in 2010, she married once more, this time in the Bronx. barrientos is accused of \"offering a false instrument for filing in the first degree\" she pleaded not guilty to two counts of \"offering a false instrument for filing in the first degree\" the marriages were part of an immigration scam, she says. in total, barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002. she is believed to still be married to four men, and at one time, she was married to eight men at once. the case was referred to the Bronx District Attorney's Office. seven of the men are from so-called \"red-flagged\" countries. it is unclear whether any of the men will be prosecuted. barrientos faces up to four years in prison if convicted. her eighth husband, Rashid Rajput, was deported to his native Pakistan in 2006. if convicted, she faces up to four years in prison.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.9347826086956522, recall=0.45989304812834225, fmeasure=0.6164874551971327), mid=Score(precision=0.9347826086956522, recall=0.45989304812834225, fmeasure=0.6164874551971327), high=Score(precision=0.9347826086956522, recall=0.45989304812834225, fmeasure=0.6164874551971327)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.7213114754098361, recall=0.353887399463807, fmeasure=0.4748201438848921), mid=Score(precision=0.7213114754098361, recall=0.353887399463807, fmeasure=0.4748201438848921), high=Score(precision=0.7213114754098361, recall=0.353887399463807, fmeasure=0.4748201438848921)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.7554347826086957, recall=0.3716577540106952, fmeasure=0.4982078853046595), mid=Score(precision=0.7554347826086957, recall=0.3716577540106952, fmeasure=0.4982078853046595), high=Score(precision=0.7554347826086957, recall=0.3716577540106952, fmeasure=0.4982078853046595)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.7554347826086957, recall=0.3716577540106952, fmeasure=0.4982078853046595), mid=Score(precision=0.7554347826086957, recall=0.3716577540106952, fmeasure=0.4982078853046595), high=Score(precision=0.7554347826086957, recall=0.3716577540106952, fmeasure=0.4982078853046595))}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n =512\n",
        "bank = [dataset[i:i+n] for i in range(0, len(dataset), n)]\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "\n",
        "summary = []\n",
        "for data in bank:\n",
        "    #inputs, references = batch\n",
        "    inputs = tokenizer(\"summarize: \" + data, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary.append(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "\n",
        "\n",
        "    #print(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "result  = \"\".join(summary)\n",
        "result = result.replace(r'<pad>','').replace(r'</s>','')\n",
        "\n",
        "print(result)\n",
        "\n",
        "metric.compute(predictions = [result], references = [dataset])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMu4Uv3BURLW",
        "outputId": "4cdfddc1-217e-4bcc-b212-a4c556ff00ac"
      },
      "id": "CMu4Uv3BURLW",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " a year later, she got married again in westchester county, new york. only 18 days after that marriage, she got hitched yet again. in 2010, she married once more, this time in the Bronx. barrientos, now 39, is facing two criminal counts of \"offering a false instrument\" referring to her false statements on the 2010 marriage license application. the marriages were part of an immigration scam, her attorney says. in total, barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002. she is believed to still be married to four men, and at one time, she was married to eight men at once. seven of the men are from so-called \"red-flagged\" countries. seven of the men are from so-called \"red-flagged\" countries. the case was referred to the district attorney's office. if convicted, barrientos faces up to four years in prison. if convicted, barrientos faces up to four years in prison. next court appearance is scheduled for may 18.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.9112426035502958, recall=0.4117647058823529, fmeasure=0.567219152854512), mid=Score(precision=0.9112426035502958, recall=0.4117647058823529, fmeasure=0.567219152854512), high=Score(precision=0.9112426035502958, recall=0.4117647058823529, fmeasure=0.567219152854512)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.8035714285714286, recall=0.36193029490616624, fmeasure=0.4990757855822551), mid=Score(precision=0.8035714285714286, recall=0.36193029490616624, fmeasure=0.4990757855822551), high=Score(precision=0.8035714285714286, recall=0.36193029490616624, fmeasure=0.4990757855822551)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.834319526627219, recall=0.3770053475935829, fmeasure=0.5193370165745856), mid=Score(precision=0.834319526627219, recall=0.3770053475935829, fmeasure=0.5193370165745856), high=Score(precision=0.834319526627219, recall=0.3770053475935829, fmeasure=0.5193370165745856)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.834319526627219, recall=0.3770053475935829, fmeasure=0.5193370165745856), mid=Score(precision=0.834319526627219, recall=0.3770053475935829, fmeasure=0.5193370165745856), high=Score(precision=0.834319526627219, recall=0.3770053475935829, fmeasure=0.5193370165745856))}"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0ZCaGaUEURIJ"
      },
      "id": "0ZCaGaUEURIJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_string = StringIO()\n",
        "with open(\"data/summary_two.pdf\" , \"rb\") as in_file:\n",
        "    parser = PDFParser(in_file)\n",
        "    doc = PDFDocument(parser)\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    device =  TextConverter(rsrcmgr, output_string, laparams= LAParams())\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    \n",
        "    for page in PDFPage.create_pages(doc):\n",
        "        interpreter.process_page(page)\n",
        "        #print('*' * 100)\n",
        "        \n",
        "print(output_string.getvalue())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z1R5ZwSURFU",
        "outputId": "909dfba9-1a83-4245-d6ef-6436b2d0cbdb"
      },
      "id": "3Z1R5ZwSURFU",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Despite the fact that our planet is habitable only because most of it is\n",
            "composed of water, it is the oceans that are the most immediately threatened\n",
            "part of the earth.\n",
            "\n",
            "It was in the oceans that life ﬁrst began to stir, shielded by the waters from the\n",
            "sun’s irresistible radiation. It was from the oceans that planets and animals\n",
            "emerged to colonize the land surface of the planet.  It is the oceans today that\n",
            "provide the water vapour which, drawn up by the sun, falls upon the earth in\n",
            "harvest-bringing, life-sustaining rain. The ocean is a major provider of the\n",
            "oxygen released by its plankton for the beneﬁt of all the species of land, air and\n",
            "sea – breathing with lungs and gills.\n",
            "\n",
            "Without special qualities for holding heat, much of the earth would be\n",
            "uninhabitable.\n",
            "\n",
            "The oceans are the coolants of the tropics, the bringers of warm currents to\n",
            "cold regions, the universal moderators of temperature throughout the globe.\n",
            "\n",
            "The oceans are also indispensable to man because they ﬁrst created the\n",
            "worldwide currents of sea-borne trade which have steadily drawn our planet\n",
            "into a single economic system. They still produce protein. In 1996, sixty-three\n",
            "million metric tons of ﬁsh came from the sea, estimated to be approximately\n",
            "one-ﬁfth of the ocean’s production.\n",
            "\n",
            "Fish, if turned directly to human use, could make up a large part of the protein\n",
            "diet required for the world’s children, especially those in developing countries,\n",
            "at a very low cost. But, in one of the world economy’s most unacceptable\n",
            "diversions of resources, ﬁfty percent of the harvest from the oceans is\n",
            "converted to ﬁsh meal which today ends up feeding pigs and chickens in\n",
            "developed countries. It is very sad that ‘developed’ animal pets have the chance\n",
            "of a better diet than very many ‘developing’ babies.\n",
            "\n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = output_string.getvalue()\n",
        "text = text[:-4]\n",
        "dataset = text.replace('\\n', ' ').replace('\\\\', '').replace('\\n\\n', '').replace('\\x0c','').replace('\\x00', 'ff')\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "wbh8fC8nURCL",
        "outputId": "3cc6ba61-7b5d-4142-8308-aff7fdda3180"
      },
      "id": "wbh8fC8nURCL",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Despite the fact that our planet is habitable only because most of it is composed of water, it is the oceans that are the most immediately threatened part of the earth.  It was in the oceans that life ﬁrst began to stir, shielded by the waters from the sun’s irresistible radiation. It was from the oceans that planets and animals emerged to colonize the land surface of the planet.  It is the oceans today that provide the water vapour which, drawn up by the sun, falls upon the earth in harvest-bringing, life-sustaining rain. The ocean is a major provider of the oxygen released by its plankton for the beneﬁt of all the species of land, air and sea – breathing with lungs and gills.  Without special qualities for holding heat, much of the earth would be uninhabitable.  The oceans are the coolants of the tropics, the bringers of warm currents to cold regions, the universal moderators of temperature throughout the globe.  The oceans are also indispensable to man because they ﬁrst created the worldwide currents of sea-borne trade which have steadily drawn our planet into a single economic system. They still produce protein. In 1996, sixty-three million metric tons of ﬁsh came from the sea, estimated to be approximately one-ﬁfth of the ocean’s production.  Fish, if turned directly to human use, could make up a large part of the protein diet required for the world’s children, especially those in developing countries, at a very low cost. But, in one of the world economy’s most unacceptable diversions of resources, ﬁfty percent of the harvest from the oceans is converted to ﬁsh meal which today ends up feeding pigs and chickens in developed countries. It is very sad that ‘developed’ animal pets have the chance of a better diet than very many ‘developing’ babies'"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n =512\n",
        "bank = [dataset[i:i+n] for i in range(0, len(dataset), n)]\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "\n",
        "summary = []\n",
        "for data in bank:\n",
        "    #inputs, references = batch\n",
        "    inputs = tokenizer(\"summarize: \" + data, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary.append(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "\n",
        "\n",
        "    #print(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "result  = \"\".join(summary)\n",
        "result = result.replace(r'<pad>','').replace(r'</s>','')\n",
        "\n",
        "print(result)\n",
        "\n",
        "metric.compute(predictions = [result], references = [dataset])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR_pvzfEUQ5r",
        "outputId": "23360f80-4fdf-4d60-8d17-103c490794a7"
      },
      "id": "TR_pvzfEUQ5r",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " the oceans provide the water vapour which, drawn up by the sun, falls upon the earth in harvest-bringing, life-bringing, harvest-bringing, life-sustaining ways. it is the oceans today that provide the water vapour which, drawn up by the sun, falls upon the earth in harvest-bringing, life-sustaining ways. the oceans are the coolants of the tropics, the bringers of warm currents to cold regions, the universal moderators of temperature throughout the globe. without special qualities for holding heat, much of the earth would be uninhabitable. in 1996, sixty-three million metric tons of fish came from the sea. fish could make up a large part of the protein diet required for the world’s children, especially those in developing countries, at a very low cost. rcent of the harvest from the oceans is converted to fish meal which today ends up feeding pigs and chickens in developed countries. it is very sad that ‘developed’ animal pets have the chance of a better diet than very many ‘developing’ babies.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.9011627906976745, recall=0.4983922829581994, fmeasure=0.6418219461697724), mid=Score(precision=0.9011627906976745, recall=0.4983922829581994, fmeasure=0.6418219461697724), high=Score(precision=0.9011627906976745, recall=0.4983922829581994, fmeasure=0.6418219461697724)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.8011695906432749, recall=0.44193548387096776, fmeasure=0.5696465696465697), mid=Score(precision=0.8011695906432749, recall=0.44193548387096776, fmeasure=0.5696465696465697), high=Score(precision=0.8011695906432749, recall=0.44193548387096776, fmeasure=0.5696465696465697)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.7965116279069767, recall=0.4405144694533762, fmeasure=0.567287784679089), mid=Score(precision=0.7965116279069767, recall=0.4405144694533762, fmeasure=0.567287784679089), high=Score(precision=0.7965116279069767, recall=0.4405144694533762, fmeasure=0.567287784679089)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.7965116279069767, recall=0.4405144694533762, fmeasure=0.567287784679089), mid=Score(precision=0.7965116279069767, recall=0.4405144694533762, fmeasure=0.567287784679089), high=Score(precision=0.7965116279069767, recall=0.4405144694533762, fmeasure=0.567287784679089))}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n =512\n",
        "bank = [dataset[i:i+n] for i in range(0, len(dataset), n)]\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "\n",
        "summary = []\n",
        "for data in bank:\n",
        "    #inputs, references = batch\n",
        "    inputs = tokenizer(\"summarize: \" + data, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary.append(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "\n",
        "\n",
        "    #print(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "result  = \"\".join(summary)\n",
        "result = result.replace(r'<pad>','').replace(r'</s>','')\n",
        "\n",
        "print(result)\n",
        "\n",
        "metric.compute(predictions = [result], references = [dataset])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9nlZVEpWnLf",
        "outputId": "d57f2066-7051-41ea-d59c-217b5955fcaa"
      },
      "id": "r9nlZVEpWnLf",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " the oceans today provide the water vapour which falls upon the earth in harvest-bringing, life-bringing. it was in the oceans that life first began to stir, shielded by the waters from the sun’s irresistible radiation. the oceans are the coolants of the tropics, the bringers of warm currents to cold regions, the universal moderators of temperature throughout the globe. the oceans are also indispensable to man because they first created the worldwide currents of sebastian. in 1996, sixty-three million metric tons of fish came from the sea. fish could make up a large part of the protein diet required for the world’s children at a very low cost. rcent of the harvest from the oceans is converted to fish meal which ends up feeding pigs and chickens in developed countries. it is sad that ‘developed’ animal pets have the chance of a better diet than very many ‘developing’ babies.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.9545454545454546, recall=0.47266881028938906, fmeasure=0.6322580645161291), mid=Score(precision=0.9545454545454546, recall=0.47266881028938906, fmeasure=0.6322580645161291), high=Score(precision=0.9545454545454546, recall=0.47266881028938906, fmeasure=0.6322580645161291)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.8562091503267973, recall=0.42258064516129035, fmeasure=0.5658747300215983), mid=Score(precision=0.8562091503267973, recall=0.42258064516129035, fmeasure=0.5658747300215983), high=Score(precision=0.8562091503267973, recall=0.42258064516129035, fmeasure=0.5658747300215983)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.8831168831168831, recall=0.43729903536977494, fmeasure=0.5849462365591398), mid=Score(precision=0.8831168831168831, recall=0.43729903536977494, fmeasure=0.5849462365591398), high=Score(precision=0.8831168831168831, recall=0.43729903536977494, fmeasure=0.5849462365591398)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.8831168831168831, recall=0.43729903536977494, fmeasure=0.5849462365591398), mid=Score(precision=0.8831168831168831, recall=0.43729903536977494, fmeasure=0.5849462365591398), high=Score(precision=0.8831168831168831, recall=0.43729903536977494, fmeasure=0.5849462365591398))}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_string = StringIO()\n",
        "with open(\"data/summary_three.pdf\" , \"rb\") as in_file:\n",
        "    parser = PDFParser(in_file)\n",
        "    doc = PDFDocument(parser)\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    device =  TextConverter(rsrcmgr, output_string, laparams= LAParams())\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    \n",
        "    for page in PDFPage.create_pages(doc):\n",
        "        interpreter.process_page(page)\n",
        "        #print('*' * 100)\n",
        "        \n",
        "print(output_string.getvalue())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er9bgX78Wxoa",
        "outputId": "e81d8d34-a503-415b-b8f3-742d38d07122"
      },
      "id": "er9bgX78Wxoa",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It has been the custom of historians to divide the factors for wars into\n",
            "immediate and underlying causes. Among these underlying causes, the\n",
            "economic factor is generally placed at the head of the list. Indeed, the most\n",
            "important of these was the industrial and commercial rivalry between\n",
            "Germany and Great Britain.\n",
            "\n",
            "Germany, after its uniﬁcation in 1871, went through a period of economic\n",
            "miracle. By 1914, she was producing more iron and steel than Britain and\n",
            "France combined. In chemicals, in dye, and in the manufacture of scientiﬁc\n",
            "equipment she led the world. The products of her industries were crowding\n",
            "British manufactures in nearly every market for continental Europe, in the Far\n",
            "East and in Britain itself.\n",
            "\n",
            "There is evidence that certain interests in Great Britain were becoming\n",
            "seriously alarmed over the menace of German competition.\n",
            "\n",
            "There seemed to be a strong conviction that Germany was waging deliberate\n",
            "and deadly economic warfare upon Britain to capture her market by unfair\n",
            "methods. Thus, for Britain to allow Germany to be victorious in this struggle\n",
            "would mean the destruction of her prosperity and a grave threat to her national\n",
            "existence.\n",
            "\n",
            "There are indications that the French also were alarmed by the German\n",
            "industrial expansion. In 1870, France had lost possession of the expensive iron\n",
            "and coal deposit of Lorraine, which had gone to swell the industrial growth of\n",
            "Germany. To be sure, the French had plenty of iron left in the Briery Fields, but\n",
            "they were afraid that their enemy might eventually reach out and grab these\n",
            "too. Besides, France was under necessity of importing coal and this galled her\n",
            "pride almost as much as the loss of the iron.\n",
            "\n",
            "In addition, the Russian ambition to gain control of Constantinople and other\n",
            "portions of Turkish territory conﬂicted with German plans for reserving the\n",
            "Turkish Empire as their happy hunting ground of commercial privilege. Then\n",
            "Russia and Austria a close ally of Germany were rivals for a monopoly of trade\n",
            "with the Balkan kingdoms of Serbia, Romania, Bulgaria and Greece.\n",
            "\n",
            "\f\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = output_string.getvalue()\n",
        "text = text[:-4]\n",
        "dataset = text.replace('\\n', ' ').replace('\\\\', '').replace('\\n\\n', '').replace('\\x0c','').replace('\\x00', 'ff')\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "v5DFl4NPXRjw",
        "outputId": "7090191f-0e3a-4cd1-d85b-4c1ac12bb55a"
      },
      "id": "v5DFl4NPXRjw",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'It has been the custom of historians to divide the factors for wars into immediate and underlying causes. Among these underlying causes, the economic factor is generally placed at the head of the list. Indeed, the most important of these was the industrial and commercial rivalry between Germany and Great Britain.  Germany, after its uniﬁcation in 1871, went through a period of economic miracle. By 1914, she was producing more iron and steel than Britain and France combined. In chemicals, in dye, and in the manufacture of scientiﬁc equipment she led the world. The products of her industries were crowding British manufactures in nearly every market for continental Europe, in the Far East and in Britain itself.  There is evidence that certain interests in Great Britain were becoming seriously alarmed over the menace of German competition.  There seemed to be a strong conviction that Germany was waging deliberate and deadly economic warfare upon Britain to capture her market by unfair methods. Thus, for Britain to allow Germany to be victorious in this struggle would mean the destruction of her prosperity and a grave threat to her national existence.  There are indications that the French also were alarmed by the German industrial expansion. In 1870, France had lost possession of the expensive iron and coal deposit of Lorraine, which had gone to swell the industrial growth of Germany. To be sure, the French had plenty of iron left in the Briery Fields, but they were afraid that their enemy might eventually reach out and grab these too. Besides, France was under necessity of importing coal and this galled her pride almost as much as the loss of the iron.  In addition, the Russian ambition to gain control of Constantinople and other portions of Turkish territory conﬂicted with German plans for reserving the Turkish Empire as their happy hunting ground of commercial privilege. Then Russia and Austria a close ally of Germany were rivals for a monopoly of trade with the Balkan kingdoms of Serbia, Romania, Bulgaria and Greece.'"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n =512\n",
        "bank = [dataset[i:i+n] for i in range(0, len(dataset), n)]\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "\n",
        "summary = []\n",
        "for data in bank:\n",
        "    #inputs, references = batch\n",
        "    inputs = tokenizer(\"summarize: \" + data, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=150, min_length=40,length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary.append(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "\n",
        "\n",
        "    #print(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "result  = \"\".join(summary)\n",
        "result = result.replace(r'<pad>','').replace(r'</s>','')\n",
        "\n",
        "print(result)\n",
        "\n",
        "metric.compute(predictions = [result], references = [dataset])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4Bp1QuUXZ1G",
        "outputId": "4a7315ab-4a63-478f-8a7a-44dafe5c93aa"
      },
      "id": "K4Bp1QuUXZ1G",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " the industrial and commercial rivalry between Germany and Great Britain. by 1914, she was producing more iron and steel than Britain and France combined. by 1914, she was producing more iron and steel than Britain and France combined. david rothkopf: her industries were crowding British manufactures in nearly every market. he says there seemed to be a conviction that Germany was waging deadly economic warfare. rothkopf: for Britain to teeter on the edge of German competition, it would be a mistake. he says it would be a mistake for the u.s. to rely on british manufacturers. frida ghitis: o allow germany to be victorious in this struggle would mean the destruction of her prosperity. ghitis: there are indications that the french also were alarmed by the German industrial expansion. ghitis: o allow Germany to be victorious would mean the destruction of her national existence. ghitis: o allow Germany to be victorious in this struggle would mean the destruction of her prosperity. frida ghitis: besides, France was under necessity of importing coal and this galled her pride. ghitis: besides, Russia and Austria a close ally of Germany were rivals for a monopoly of trade with the Balkan kingdoms. ghitis: then Russia and Austria a close ally of Germany were rivals for a monopoly of trade with the kingdoms of Serbia, Romania, Bulgaria and Grec. cnn.com/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/sce/s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.5954198473282443, recall=0.4588235294117647, fmeasure=0.5182724252491694), mid=Score(precision=0.5954198473282443, recall=0.4588235294117647, fmeasure=0.5182724252491694), high=Score(precision=0.5954198473282443, recall=0.4588235294117647, fmeasure=0.5182724252491694)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.44061302681992337, recall=0.3392330383480826, fmeasure=0.38333333333333336), mid=Score(precision=0.44061302681992337, recall=0.3392330383480826, fmeasure=0.38333333333333336), high=Score(precision=0.44061302681992337, recall=0.3392330383480826, fmeasure=0.38333333333333336)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.4961832061068702, recall=0.38235294117647056, fmeasure=0.4318936877076412), mid=Score(precision=0.4961832061068702, recall=0.38235294117647056, fmeasure=0.4318936877076412), high=Score(precision=0.4961832061068702, recall=0.38235294117647056, fmeasure=0.4318936877076412)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.4961832061068702, recall=0.38235294117647056, fmeasure=0.4318936877076412), mid=Score(precision=0.4961832061068702, recall=0.38235294117647056, fmeasure=0.4318936877076412), high=Score(precision=0.4961832061068702, recall=0.38235294117647056, fmeasure=0.4318936877076412))}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n =512\n",
        "bank = [dataset[i:i+n] for i in range(0, len(dataset), n)]\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "\n",
        "summary = []\n",
        "for data in bank:\n",
        "    #inputs, references = batch\n",
        "    inputs = tokenizer(\"summarize: \" + data, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary.append(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "\n",
        "\n",
        "    #print(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "result  = \"\".join(summary)\n",
        "result = result.replace(r'<pad>','').replace(r'</s>','')\n",
        "\n",
        "print(result)\n",
        "\n",
        "metric.compute(predictions = [result], references = [dataset])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThFdcmb-Xgg4",
        "outputId": "18a17301-b69a-4d9d-88a7-e8476e21cdef"
      },
      "id": "ThFdcmb-Xgg4",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " it has been the custom of historians to divide the factors for wars into immediate and underlying causes. the most important of these was the industrial and commercial rivalry between Germany and Great Britain. by 1914, she was producing more iron and steel than Britain and France combined. the products of her industries crowding British manufactures in nearly every market for continental Europe, in the far East and in Britain itself. there seems to be a strong conviction that Germany was waging deliberate and deadly economic warfare upon Britain to capture her market by unfair methods. in 1870, France had lost possession of the expensive iron and coal deposit of Lorraine. in 1870, the French had plenty of iron left in the Briery Fields. but they were afraid that their enemy might eventually reach ousted. france was under necessity of importing coal and this galled her pride almost as much as the loss of the iron. the Russian ambition to gain control of Constantinople conflicted with german plans for reserving the Turkish Empire as their happy hunting ground of commercial privilege. ece. ece. ece. ece. ece. ece. ece. ece. ece. ece.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.9267015706806283, recall=0.5205882352941177, fmeasure=0.6666666666666666), mid=Score(precision=0.9267015706806283, recall=0.5205882352941177, fmeasure=0.6666666666666666), high=Score(precision=0.9267015706806283, recall=0.5205882352941177, fmeasure=0.6666666666666666)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.8736842105263158, recall=0.4896755162241888, fmeasure=0.6275992438563327), mid=Score(precision=0.8736842105263158, recall=0.4896755162241888, fmeasure=0.6275992438563327), high=Score(precision=0.8736842105263158, recall=0.4896755162241888, fmeasure=0.6275992438563327)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.9214659685863874, recall=0.5176470588235295, fmeasure=0.6629001883239172), mid=Score(precision=0.9214659685863874, recall=0.5176470588235295, fmeasure=0.6629001883239172), high=Score(precision=0.9214659685863874, recall=0.5176470588235295, fmeasure=0.6629001883239172)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.9214659685863874, recall=0.5176470588235295, fmeasure=0.6629001883239172), mid=Score(precision=0.9214659685863874, recall=0.5176470588235295, fmeasure=0.6629001883239172), high=Score(precision=0.9214659685863874, recall=0.5176470588235295, fmeasure=0.6629001883239172))}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9mjzdkKRXpdd"
      },
      "id": "9mjzdkKRXpdd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_string = StringIO()\n",
        "with open(\"data/summary_four.pdf\" , \"rb\") as in_file:\n",
        "    parser = PDFParser(in_file)\n",
        "    doc = PDFDocument(parser)\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    device =  TextConverter(rsrcmgr, output_string, laparams= LAParams())\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    \n",
        "    for page in PDFPage.create_pages(doc):\n",
        "        interpreter.process_page(page)\n",
        "        #print('*' * 100)\n",
        "        \n",
        "print(output_string.getvalue())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r2C1mwiX0SH",
        "outputId": "6ea6d5f7-ed40-4ad2-eadb-7de928371619"
      },
      "id": "7r2C1mwiX0SH",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mining ranks among the world’s most destructive industries. Yet mineral\n",
            "extraction and processing are absent in most discussions of global\n",
            "environmental threats. Governmental and private analyses have focused only\n",
            "on increasing mineral supplies.\n",
            "\n",
            "Each year, mining strips some 28 billion tons of material from the earth. This\n",
            "is more than what is removed by the natural erosion of all the earth’s rivers.\n",
            "Worldwide, mining and smelting generate an estimated 2.7 billion tons of\n",
            "processing waste each year, much of it hazardous dwarﬁng the more familiar\n",
            "municipal waste. Smelter pollution has created biological wastelands as large\n",
            "as 10, 000 hectares and pumped some eight percent of the total worldwide\n",
            "emissions of sulphur dioxide, a major contributor to acid rain, into the\n",
            "atmosphere.\n",
            "\n",
            "Mining could also cause more damaging deforestation than bad farming\n",
            "practices in certain parts of the world. For example, smelters at a single iron\n",
            "mine in Brazil will require enough fuelwood to deforest 50,000 hectares of\n",
            "tropical forest each year.\n",
            "\n",
            "Mining has been poorly regulated even in wealthy industrialized nations. While\n",
            "many governments subsidize mineral production, few enact or enforce strict\n",
            "environmental regulations for mining operations. As a result, not only are\n",
            "many mining activities more environmentally destructive than need be, but\n",
            "prices of minerals do not include their full environmental cost. Today’s low\n",
            "mineral prices reﬂect only the immediate economics of extraction and\n",
            "destruction. They fail to consider the full costs of eroded land, dammed or\n",
            "polluted rivers and displacement of people unlucky enough to live atop mineral\n",
            "deposits. In light of this, governments should remove subsidies provided for\n",
            "mining virgin minerals.\n",
            "\n",
            "The devastating e\u0000ects of the industry are particularly severe in the developing\n",
            "countries which have been producing a substantial portion of the world’s\n",
            "mineral supplies, although they use relatively little. This is because\n",
            "environmental controls tend to be weak or non-existent in these countries.\n",
            "What makes their situation more pathetic is that many of them are among the\n",
            "world’s poorest nations.\n",
            "\n",
            "\fContrary to popular belief, the people of most mineral–exporting countries\n",
            "gain little from mining. Expensive investment in equipment and infrastructure\n",
            "combined with falling world mineral prices, especially during the eighties, has\n",
            "made these countries some of the world’s most heavily indebted.\n",
            "\n",
            "While the world appears in little danger of running out of most non-fuel\n",
            "minerals, it is obvious that the planet cannot a\u0000ord the human and ecological\n",
            "price of its growing appetite for minerals. It will therefore be wise to satisfy\n",
            "human needs with smaller amounts of virgin minerals. It will also work for our\n",
            "good if we increase recycling of materials, and make metal-based products\n",
            "more durable and easier to repair.\n",
            "\n",
            "\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = output_string.getvalue()\n",
        "text = text[:-4]\n",
        "dataset = text.replace('\\n', ' ').replace('\\\\', '').replace('\\n\\n', '').replace('\\x0c','').replace('\\x00', 'ff')\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "1R6om-RHX7L8",
        "outputId": "c2775dbd-776b-42ae-bc86-41a8c9752cc0"
      },
      "id": "1R6om-RHX7L8",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Mining ranks among the world’s most destructive industries. Yet mineral extraction and processing are absent in most discussions of global environmental threats. Governmental and private analyses have focused only on increasing mineral supplies.  Each year, mining strips some 28 billion tons of material from the earth. This is more than what is removed by the natural erosion of all the earth’s rivers. Worldwide, mining and smelting generate an estimated 2.7 billion tons of processing waste each year, much of it hazardous dwarﬁng the more familiar municipal waste. Smelter pollution has created biological wastelands as large as 10, 000 hectares and pumped some eight percent of the total worldwide emissions of sulphur dioxide, a major contributor to acid rain, into the atmosphere.  Mining could also cause more damaging deforestation than bad farming practices in certain parts of the world. For example, smelters at a single iron mine in Brazil will require enough fuelwood to deforest 50,000 hectares of tropical forest each year.  Mining has been poorly regulated even in wealthy industrialized nations. While many governments subsidize mineral production, few enact or enforce strict environmental regulations for mining operations. As a result, not only are many mining activities more environmentally destructive than need be, but prices of minerals do not include their full environmental cost. Today’s low mineral prices reﬂect only the immediate economics of extraction and destruction. They fail to consider the full costs of eroded land, dammed or polluted rivers and displacement of people unlucky enough to live atop mineral deposits. In light of this, governments should remove subsidies provided for mining virgin minerals.  The devastating effects of the industry are particularly severe in the developing countries which have been producing a substantial portion of the world’s mineral supplies, although they use relatively little. This is because environmental controls tend to be weak or non-existent in these countries. What makes their situation more pathetic is that many of them are among the world’s poorest nations.  Contrary to popular belief, the people of most mineral–exporting countries gain little from mining. Expensive investment in equipment and infrastructure combined with falling world mineral prices, especially during the eighties, has made these countries some of the world’s most heavily indebted.  While the world appears in little danger of running out of most non-fuel minerals, it is obvious that the planet cannot afford the human and ecological price of its growing appetite for minerals. It will therefore be wise to satisfy human needs with smaller amounts of virgin minerals. It will also work for our good if we increase recycling of materials, and make metal-based products more durable and easier to repair'"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n =512\n",
        "bank = [dataset[i:i+n] for i in range(0, len(dataset), n)]\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "\n",
        "summary = []\n",
        "for data in bank:\n",
        "    #inputs, references = batch\n",
        "    inputs = tokenizer(\"summarize: \" + data, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary.append(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "\n",
        "\n",
        "    #print(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "result  = \"\".join(summary)\n",
        "result = result.replace(r'<pad>','').replace(r'</s>','')\n",
        "\n",
        "print(result)\n",
        "\n",
        "metric.compute(predictions = [result], references = [dataset])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEvFWeXZX--i",
        "outputId": "91bbfcb4-4fd2-4cc7-dc56-332563912be0"
      },
      "id": "JEvFWeXZX--i",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " each year, mining strips some 28 billion tons of material from the earth. this is more than what is removed by the natural erosion of all the earth’s rivers. mining and smelting generate an estimated 2.7 billion tons of processing waste each year. smelter pollution has created biological wastelands as large as 10, 000 hectares. mining could cause more damaging deforestation than bad farming practices. mining could cause more damaging deforestation than bad farming practices. mining has been poorly regulated even in wealthy industrialized nations. many governments subsidize mineral production, but few enact or enforce strict environmental regulations. prices of minerals do not include their full environmental cost. osts of eroded land, dammed or polluted rivers and displacement of people unlucky enough to live atop mineral deposits. the devastating effects of the industry are particularly severe in the developing countries. the people of most mineral–exporting countries gain little from mining. falling world mineral prices, especially during the eighties, has made these countries some of the world’s most heavily indebted. t can afford the human and ecological price of its growing appetite for minerals. t will therefore be wise to satisfy human needs with smaller amounts of virgin minerals. it will also work for our good if we increase recycling of materials.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.9488372093023256, recall=0.4523281596452328, fmeasure=0.6126126126126126), mid=Score(precision=0.9488372093023256, recall=0.4523281596452328, fmeasure=0.6126126126126126), high=Score(precision=0.9488372093023256, recall=0.4523281596452328, fmeasure=0.6126126126126126)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.8644859813084113, recall=0.4111111111111111, fmeasure=0.5572289156626505), mid=Score(precision=0.8644859813084113, recall=0.4111111111111111, fmeasure=0.5572289156626505), high=Score(precision=0.8644859813084113, recall=0.4111111111111111, fmeasure=0.5572289156626505)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.9302325581395349, recall=0.4434589800443459, fmeasure=0.6006006006006006), mid=Score(precision=0.9302325581395349, recall=0.4434589800443459, fmeasure=0.6006006006006006), high=Score(precision=0.9302325581395349, recall=0.4434589800443459, fmeasure=0.6006006006006006)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.9302325581395349, recall=0.4434589800443459, fmeasure=0.6006006006006006), mid=Score(precision=0.9302325581395349, recall=0.4434589800443459, fmeasure=0.6006006006006006), high=Score(precision=0.9302325581395349, recall=0.4434589800443459, fmeasure=0.6006006006006006))}"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n =512\n",
        "bank = [dataset[i:i+n] for i in range(0, len(dataset), n)]\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "\n",
        "summary = []\n",
        "for data in bank:\n",
        "    #inputs, references = batch\n",
        "    inputs = tokenizer(\"summarize: \" + data, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary.append(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "\n",
        "\n",
        "    #print(tokenizer.decode(outputs[0]))\n",
        "    \n",
        "result  = \"\".join(summary)\n",
        "result = result.replace(r'<pad>','').replace(r'</s>','')\n",
        "\n",
        "print(result)\n",
        "\n",
        "metric.compute(predictions = [result], references = [dataset])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVIj5Uqlg3Lo",
        "outputId": "a72a63a5-44f6-49ed-c494-5a81cca207fc"
      },
      "id": "IVIj5Uqlg3Lo",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " each year, mining strips 28 billion tons of material from the earth. this is more than what is removed by the natural erosion of all the earth’s rivers. worldwide, mining and smelting generate an estimated 2.7 billion tons of processing waste each year. smelter pollution has created biological wastelands as large as 10, 000 hectares. mining could also cause more damaging deforestation than bad farming practices. mining could also cause more damaging deforestation than bad farming practices in certain parts of the world. mining has been poorly regulated even in wealthy industrialized nations. some governments enact or enforce strict environmental regulations for mining operations. prices of minerals do not include their full environmental cost. osts of eroded land, dammed or polluted rivers and displacement of people unlucky enough to live atop mineral deposits. governments should remove subsidies provided for mining virgin minerals. the people of most mineral–exporting countries gain little from mining. the world appears in little danger of running out of most non-fuel minerals. but the plane is obvious that the plane is in danger of running out of most non-fuel. t cannot afford the human and ecological price of its growing appetite for minerals. it will therefore be wise to satisfy human needs with smaller amounts of virgin minerals. it will also work for our good if we increase recycling of materials.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.9301310043668122, recall=0.4722838137472284, fmeasure=0.6264705882352941), mid=Score(precision=0.9301310043668122, recall=0.4722838137472284, fmeasure=0.6264705882352941), high=Score(precision=0.9301310043668122, recall=0.4722838137472284, fmeasure=0.6264705882352941)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.8245614035087719, recall=0.4177777777777778, fmeasure=0.5545722713864307), mid=Score(precision=0.8245614035087719, recall=0.4177777777777778, fmeasure=0.5545722713864307), high=Score(precision=0.8245614035087719, recall=0.4177777777777778, fmeasure=0.5545722713864307)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.8777292576419214, recall=0.44567627494456763, fmeasure=0.5911764705882353), mid=Score(precision=0.8777292576419214, recall=0.44567627494456763, fmeasure=0.5911764705882353), high=Score(precision=0.8777292576419214, recall=0.44567627494456763, fmeasure=0.5911764705882353)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.8777292576419214, recall=0.44567627494456763, fmeasure=0.5911764705882353), mid=Score(precision=0.8777292576419214, recall=0.44567627494456763, fmeasure=0.5911764705882353), high=Score(precision=0.8777292576419214, recall=0.44567627494456763, fmeasure=0.5911764705882353))}"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference\n",
        "1. What is Rouge score? [here](https://www.youtube.com/watch?v=TMshhnrEXlg)\n",
        "2. Interpreting Rouge scores [here](https://stats.stackexchange.com/questions/301626/interpreting-rouge-scores)\n",
        "3. How others tested their text summarizer after using rouge [here](https://link.springer.com/article/10.1007/s10579-017-9389-4)\n",
        "4. Understanding how to finetune Hyperparameters for model.generate [here](https://huggingface.co/blog/how-to-generate)\n",
        "5. Understanding num_beam used as an argument for model.generate [here](https://huggingface.co/blog/how-to-generate)"
      ],
      "metadata": {
        "id": "z2B1aaNVaXgi"
      },
      "id": "z2B1aaNVaXgi"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6xQheM6ZYEf6"
      },
      "id": "6xQheM6ZYEf6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "testing_tran_models_hp_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}